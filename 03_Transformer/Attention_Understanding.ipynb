{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Attention_Understanding.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMdJcCTPhlWZquBO/AXVqeg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6L87tg9Iw_MC","colab_type":"text"},"source":["# Attention的理解"]},{"cell_type":"markdown","metadata":{"id":"egS8Ro6VZEmw","colab_type":"text"},"source":["Attention 机制实质上就是一个寻址过程，通过给定一个**任务相关的查询 Query 向量 Q**，通过**计算与 Key 的注意力分布并附加在 Value 上**，从而计算 Attention Value，这个过程实际上是 Attention 缓解神经网络复杂度的体现，不需要将所有的 N 个输入都输入到神经网络进行计算，而是选择一些与任务相关的信息输入神经网络，与 RNN 中的门控机制思想类似。\n","\n","![替代文字](https://img-blog.csdnimg.cn/20200306212959524.jpg)"]},{"cell_type":"markdown","metadata":{"id":"bdb-FJUaaDzN","colab_type":"text"},"source":["Attention 机制计算过程大致可以分成三步：\n","\n","① 信息输入：将 Q，K，V 输入模型，\n","用 $X=[x_1,x_2,\\cdots,x_n]$表示输入权重向量（即数据的输入）\n","\n","\n","---\n","\n","\n","\n","② 计算注意力分布 α：通过计算 Q 和 K 进行点积计算相关度，并通过 softmax 计算分数\n","令$Q=K=V=X$，通过 softmax 计算注意力权重，$α_i=softmax(s(k_i,q))=softmax(s(x_i, q))$\n","\n","我们将$α_i$称之为注意力概率分布，$s(x_i, q)$ 为注意力打分机制，常见的有如下几种：\n","\n","- 加性模型：$s(x_i,q)=v^Ttanh(Wx_i+Uq)$  \n","- 点积模型：$s(x_i,q)=x_i^Tq$  \n","- 缩放点积模型：$s(x_i,q)={x_i^Tq}/\\sqrt{d_k}$  \n","- 双线性模型：$s(x_i,q)=x_i^TWq$  \n","\n","\n","---\n","\n","\n","\n","③ 信息加权平均：注意力分布 $α_i$来解释在上下文查询$q_i$时，第 $i$ 个信息受关注程度。\n","$att(q,X)=\\sum_{i=1}^N{α_iX_i}$"]},{"cell_type":"markdown","metadata":{"id":"bgroSdZRdi8V","colab_type":"text"},"source":["上面从attention函数得到了attention机制工作过程。现在换一个角度来理解，我们将attention机制看做**软寻址**。\n","\n","序列中每一个元素都由key(地址)和value(元素)数据对存储在存储器里，当有query=key的查询时，需要取出元素的value值(也即query查询的attention值)，与传统的寻址不一样，它不是按照地址取出值的，它是**通过计算key与query的相似度来完成寻址**,这就是所谓的软寻址。\n","\n","它可能会把所有地址(key)的值(value)取出来，上步计算出的**相似度决定了取出来值的重要程度，然后按重要程度合并value值得到attention值**，此处的合并指的是加权求和。"]},{"cell_type":"markdown","metadata":{"id":"39lDKDNrxDvy","colab_type":"text"},"source":["# 按比缩放的点积注意力"]},{"cell_type":"markdown","metadata":{"id":"n6fkSrINxtN1","colab_type":"text"},"source":["Transformer 创建了多层自注意力层（self-attetion layers）组成的堆栈，每层注意力又使用了多头注意力机制的方法，其中使用的注意力打分方法是缩放点积模型。\n","\n","<img src=\"https://tensorflow.google.cn/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n","\n","$Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V$"]},{"cell_type":"code","metadata":{"id":"F1pMM6qzzR2v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e0992bf9-49e8-495c-fabd-477971bcd1d3","executionInfo":{"status":"ok","timestamp":1586165046114,"user_tz":-480,"elapsed":854,"user":{"displayName":"qingyuan liang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTJHoGFryhWrfj2D0X8Yu7JTP_jfz9n_P4els=s64","userId":"15260138906199842493"}}},"source":["import tensorflow as tf\n","import numpy as np\n","print(tf.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["2.2.0-rc2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nrtQj1vAY4u7","colab_type":"code","colab":{}},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","    \"\"\"计算注意力权重。\n","  q, k, v 必须具有匹配的前置维度。\n","  k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n","  虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n","  但是 mask 必须能进行广播转换以便求和。\n","  \n","  参数:\n","    q: 请求的形状 == (..., seq_len_q, depth)\n","    k: 主键的形状 == (..., seq_len_k, depth)\n","    v: 数值的形状 == (..., seq_len_v, depth_v)\n","    mask: Float 张量，其形状能转换成\n","          (..., seq_len_q, seq_len_k)。默认为None。\n","    \n","  返回值:\n","    输出，注意力权重\n","  \"\"\"\n","\n","    matmul_qk = tf.matmul(q, k,\n","                          transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","\n","    # 缩放 matmul_qk\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    # 将 mask 加入到缩放的张量上。\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)\n","\n","    # softmax 在最后一个轴（seq_len_k）上归一化，因此分数\n","    # 相加等于1。\n","    attention_weights = tf.nn.softmax(scaled_attention_logits,\n","                                      axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","    return output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SWRfrHury257","colab_type":"text"},"source":["当 softmax 在 K 上进行归一化后，它的值决定了分配到 Q 的重要程度。\n","\n","输出表示注意力权重和 V（数值）向量的乘积。这确保了要关注的词保持原样，而无关的词将被清除掉。\n","\n","Q是对自身（self）输入的变换，所以称之为自注意力机制，而在传统的Attention中，Q来自于外部。"]},{"cell_type":"code","metadata":{"id":"13OcYxz3yyou","colab_type":"code","colab":{}},"source":["#计算注意力权重和输出\n","def print_out(q, k, v):\n","    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n","    print('Attention weights are:')\n","    print(temp_attn)\n","    print('Output is:')\n","    print(temp_out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2p-bpmhOzNOH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"79681f68-da5c-468d-c02c-274cc4843e76","executionInfo":{"status":"ok","timestamp":1586165059742,"user_tz":-480,"elapsed":1177,"user":{"displayName":"qingyuan liang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTJHoGFryhWrfj2D0X8Yu7JTP_jfz9n_P4els=s64","userId":"15260138906199842493"}}},"source":["np.set_printoptions(suppress=True)\n","#Key\n","temp_k = tf.constant([[10,0,0],\n","                      [0,10,0],\n","                      [0,0,10],\n","                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n","#Value\n","temp_v = tf.constant([[   1,0],\n","                      [  10,0],\n","                      [ 100,5],\n","                      [1000,6]], dtype=tf.float32)  # (4, 2)\n","\n","# 这条 `请求（query）`符合第二个`主键（key）`，即Q与第二个K最相似，\n","# 因此返回了第二个`数值（value）`。\n","temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n","print_out(temp_q, temp_k, temp_v)\n","#在注意力打分中第二个Key的得分权重是1，所以返回第二个Value"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n","Output is:\n","tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5FPK5-o41DDh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"4c05963d-32e9-4256-9bfd-c7715435e574","executionInfo":{"status":"ok","timestamp":1586165464207,"user_tz":-480,"elapsed":942,"user":{"displayName":"qingyuan liang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTJHoGFryhWrfj2D0X8Yu7JTP_jfz9n_P4els=s64","userId":"15260138906199842493"}}},"source":["# 这条请求符合重复出现的主键（第三第四个），\n","# 因此，对所有的相关数值取了平均。\n","temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n","print_out(temp_q, temp_k, temp_v)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n","Output is:\n","tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fvx0oJ-f1I1J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"a51764ab-9d65-40a2-c1bd-0fafa1da93e6","executionInfo":{"status":"ok","timestamp":1586165487876,"user_tz":-480,"elapsed":900,"user":{"displayName":"qingyuan liang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTJHoGFryhWrfj2D0X8Yu7JTP_jfz9n_P4els=s64","userId":"15260138906199842493"}}},"source":["# 这条请求符合第一和第二条主键，\n","# 因此，对它们的数值去了平均。\n","temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n","print_out(temp_q, temp_k, temp_v)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n","Output is:\n","tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ce63sfZw1MzL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"f090a2d5-ca2c-4ddf-e182-d98c840cd324","executionInfo":{"status":"ok","timestamp":1586165524474,"user_tz":-480,"elapsed":874,"user":{"displayName":"qingyuan liang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTJHoGFryhWrfj2D0X8Yu7JTP_jfz9n_P4els=s64","userId":"15260138906199842493"}}},"source":["#将所有请求一起传递。\n","temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n","print_out(temp_q, temp_k, temp_v)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","tf.Tensor(\n","[[0.  0.  0.5 0.5]\n"," [0.  1.  0.  0. ]\n"," [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n","Output is:\n","tf.Tensor(\n","[[550.    5.5]\n"," [ 10.    0. ]\n"," [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HX0o80rf1duq","colab_type":"text"},"source":["# 多头注意力机制 \n","<img src=\"https://tensorflow.google.cn/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n","\n","\n","多头注意力由四部分组成：\n","*    线性层并分拆成多头。\n","*    按比缩放的点积注意力。\n","*    多头及联。\n","*    最后一层线性层。\n","\n","\n","每个多头注意力块有三个输入：Q（请求）、K（主键）、V（数值）。这些输入经过线性（Dense）层，并分拆成多头。 \n","\n","将上面定义的 `scaled_dot_product_attention` 函数应用于每个头（进行了广播（broadcasted）以提高效率）。注意力这步必须使用一个恰当的 mask。然后将每个头的注意力输出连接起来（用`tf.transpose` 和 `tf.reshape`），并放入最后的 `Dense` 层。\n","\n","Q、K、和 V 被拆分到了多个头，而非单个的注意力头，因为多头允许模型共同注意来自不同表示空间的不同位置的信息。在分拆后，每个头部的维度减少，因此总的计算成本与有着全部维度的单个注意力头相同。"]},{"cell_type":"code","metadata":{"id":"1nr5qm3010eJ","colab_type":"code","colab":{}},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        \"\"\"分拆最后一个维度到 (num_heads, depth).\n","    转置结果使得形状为 (batch_size, num_heads, seq_len, depth)\n","    \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)  # (batch_size, seq_len, d_model)\n","        k = self.wk(k)  # (batch_size, seq_len, d_model)\n","        v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","        q = self.split_heads(\n","            q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(\n","            k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","        v = self.split_heads(\n","            v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(\n","            scaled_attention,\n","            perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(\n","            scaled_attention,\n","            (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(\n","            concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rcao9xpd2IX1","colab_type":"text"},"source":["创建一个 `MultiHeadAttention` 层进行尝试。在序列中的每个位置 `y`，`MultiHeadAttention` 在序列中的所有其他位置运行所有8个注意力头，在每个位置y，返回一个新的同样长度的向量。"]},{"cell_type":"code","metadata":{"id":"spYGPXQo2JJy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"00f5b3f5-3b46-489d-89d9-5b2ae1c3bb11","executionInfo":{"status":"ok","timestamp":1586166021283,"user_tz":-480,"elapsed":830,"user":{"displayName":"qingyuan liang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTJHoGFryhWrfj2D0X8Yu7JTP_jfz9n_P4els=s64","userId":"15260138906199842493"}}},"source":["#参数：嵌入维度512；8个注意力头\n","#输入参数：1句话；长度60；嵌入维度512；\n","#输出：输出维度(1, 60, 512)；\n","#注意力维度：[1, 8, 60, 60] （8个头中的每一个，Q*K^T=[60,64]*[60*64]^T=[60,64]*[64*60]=[60,60]）\n","temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n","y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n","out, attn = temp_mha(y, k=y, q=y, mask=None)\n","out.shape, attn.shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"]},"metadata":{"tags":[]},"execution_count":10}]}]}